{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45484d2a",
   "metadata": {},
   "source": [
    "# Car Recognition Model Deployment\n",
    "\n",
    "This notebook documents the process of saving and deploying our car recognition model as an API using Docker, Kubernetes, and Google Cloud. We'll walk through all the necessary steps from exporting the trained model to deploying it as a scalable API service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabc883",
   "metadata": {},
   "source": [
    "## Step 1: Export the Trained Model\n",
    "\n",
    "First, we need to export our trained model to a format suitable for production. We'll use the `export_model.py` script which handles loading the trained weights and saving the model in TensorFlow's SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25990254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to export the model (or use terminal: python export_model.py --model-type transfer)\n",
    "!python export_model.py --model-type transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5821bc",
   "metadata": {},
   "source": [
    "This script performs several key actions:\n",
    "\n",
    "1. Rebuilds the model architecture based on the specified type (transfer learning with ResNet50V2)\n",
    "2. Loads the trained weights from the checkpoint files\n",
    "3. Saves the model in TensorFlow's SavedModel format\n",
    "4. Exports class names to a JSON file for prediction mapping\n",
    "\n",
    "The exported model is saved in `saved_model/transfer/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cbf9d",
   "metadata": {},
   "source": [
    "## Step 2: Build the Flask API Server\n",
    "\n",
    "Now we need a REST API server to serve predictions from our model. We have created an `app.py` file that uses Flask to set up endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b12940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key parts of the Flask API server\n",
    "!head -n 30 app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d03a21",
   "metadata": {},
   "source": [
    "The Flask API server provides:\n",
    "\n",
    "- `/predict` endpoint that accepts image uploads and returns car make/model predictions\n",
    "- `/health` endpoint for Kubernetes health checks\n",
    "- Proper error handling and image preprocessing\n",
    "- Loading and using the saved TensorFlow model\n",
    "\n",
    "You can test the API locally before containerization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell would run the Flask app locally (commented out as it blocks notebook execution)\n",
    "# !python app.py\n",
    "\n",
    "# In a separate terminal, you could test with:\n",
    "# curl -X POST -F \"file=@cars_test/cars_test/00001.jpg\" http://localhost:5000/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b0de4",
   "metadata": {},
   "source": [
    "## Step 3: Containerize the Application with Docker\n",
    "\n",
    "Next, we will containerize our application using Docker. The Dockerfile defines the environment and dependencies required to run our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Dockerfile\n",
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458b5ee",
   "metadata": {},
   "source": [
    "The Dockerfile includes:\n",
    "\n",
    "1. A TensorFlow base image \n",
    "2. Installation of system and Python dependencies\n",
    "3. Copying of application code and the saved model\n",
    "4. Environment variable configuration\n",
    "5. Command to run the Flask application\n",
    "\n",
    "To build and test the Docker image locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4855dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Docker image\n",
    "# !docker build -t car-recognition-api:latest .\n",
    "\n",
    "# Run the container locally\n",
    "# !docker run -p 8080:8080 car-recognition-api:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c70a62",
   "metadata": {},
   "source": [
    "## Step 4: Kubernetes Configuration\n",
    "\n",
    "For deploying to Kubernetes, we've prepared configuration files that define our deployment, service, and horizontal pod autoscaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Kubernetes deployment configuration\n",
    "!cat kubernetes/deployment.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a79e4a",
   "metadata": {},
   "source": [
    "The Kubernetes configuration includes:\n",
    "\n",
    "1. A deployment with resource limits and requests\n",
    "2. A service to expose the API to external traffic\n",
    "3. A horizontal pod autoscaler to scale based on CPU utilization\n",
    "4. Health check probes to ensure container stability\n",
    "\n",
    "Note that `[PROJECT_ID]` in the deployment file needs to be replaced with your actual Google Cloud project ID before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c568",
   "metadata": {},
   "source": [
    "## Step 5: Google Cloud Platform Deployment\n",
    "\n",
    "Finally, we deploy to Google Cloud Platform using GKE (Google Kubernetes Engine). We've created a deployment script to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dca260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the deployment script\n",
    "!cat deploy.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564b10b",
   "metadata": {},
   "source": [
    "The deployment script automates the following steps:\n",
    "\n",
    "1. Exporting the model (if not already done)\n",
    "2. Building and tagging the Docker image\n",
    "3. Pushing the image to Google Container Registry\n",
    "4. Creating a GKE cluster (if it doesn't exist)\n",
    "5. Deploying the application to Kubernetes\n",
    "6. Waiting for and displaying the external IP address\n",
    "\n",
    "To run the deployment script (after editing with your GCP project ID):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20620fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set execute permission and run the deployment script\n",
    "# !chmod +x deploy.sh\n",
    "# !./deploy.sh\n",
    "\n",
    "# Note: Before running, edit deploy.sh to set your GCP project ID and preferred region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb3409",
   "metadata": {},
   "source": [
    "## Step 6: Testing the Deployed API\n",
    "\n",
    "Once deployed, you can test the API by sending HTTP requests to the provided external IP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Replace with your actual deployed API URL\n",
    "API_URL = \"http://[YOUR_EXTERNAL_IP]/predict\"\n",
    "\n",
    "def test_car_recognition_api(image_path):\n",
    "    \"\"\"Test the deployed car recognition API with a local image\"\"\"\n",
    "    # Load and display the image\n",
    "    img = Image.open(image_path)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.axis('off')\n",
    "    plt.title('Test Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # Prepare the image file for upload\n",
    "    with open(image_path, 'rb') as f:\n",
    "        files = {'file': (os.path.basename(image_path), f, 'image/jpeg')}\n",
    "        \n",
    "        try:\n",
    "            # Send POST request to the API\n",
    "            response = requests.post(API_URL, files=files)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                result = response.json()\n",
    "                \n",
    "                # Display the top prediction\n",
    "                print(f\"Top Prediction: {result['top_prediction']['class']}\")\n",
    "                print(f\"Confidence: {result['top_prediction']['confidence']:.2f}%\")\n",
    "                \n",
    "                # Display all predictions\n",
    "                print(\"\\nAll Predictions:\")\n",
    "                for i, pred in enumerate(result['predictions']):\n",
    "                    print(f\"{i+1}. {pred['class']} - {pred['confidence']:.2f}%\")\n",
    "                    \n",
    "                return result\n",
    "            else:\n",
    "                print(f\"Error: API request failed with status code {response.status_code}\")\n",
    "                print(response.text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to API: {e}\")\n",
    "            \n",
    "# Uncomment and run this to test with one of your test images\n",
    "# test_car_recognition_api('cars_test/cars_test/00001.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba489ff",
   "metadata": {},
   "source": [
    "## Step 7: Monitoring and Scaling\n",
    "\n",
    "After deployment, you can monitor your application and scale it as needed using Google Cloud Console or kubectl commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to monitor your deployment (run these in your terminal)\n",
    "'''\n",
    "# Get all pods\n",
    "kubectl get pods\n",
    "\n",
    "# Check the horizontal pod autoscaler\n",
    "kubectl get hpa\n",
    "\n",
    "# See detailed information about the deployment\n",
    "kubectl describe deployment car-recognition-api\n",
    "\n",
    "# View logs from a specific pod (replace pod-name with actual pod name)\n",
    "kubectl logs pod-name\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d6393",
   "metadata": {},
   "source": [
    "## Step 8: Clean Up Resources\n",
    "\n",
    "When you're done with the deployment, clean up the resources to avoid unnecessary charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to clean up resources (run these in your terminal)\n",
    "'''\n",
    "# Delete the Kubernetes deployment, service, and HPA\n",
    "kubectl delete -f kubernetes/deployment.yaml\n",
    "\n",
    "# Delete the GKE cluster\n",
    "gcloud container clusters delete car-recognition-cluster --region=us-central1\n",
    "\n",
    "# Delete the container images\n",
    "gcloud container images delete gcr.io/[YOUR_PROJECT_ID]/car-recognition-api:latest --force-delete-tags\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767aa473",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have successfully deployed our car recognition model as a scalable API service using Docker, Kubernetes, and Google Cloud Platform. The deployment architecture provides:\n",
    "\n",
    "- Scalability through Kubernetes and Horizontal Pod Autoscaler\n",
    "- High availability with multiple replicas\n",
    "- Resource efficiency through containerization\n",
    "- Health monitoring for stability\n",
    "- Easy updates and rollbacks\n",
    "\n",
    "This deployment is production-ready and can handle substantial API traffic with automatic scaling based on demand."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
